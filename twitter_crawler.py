import aiohttp
import asyncio
from bs4 import BeautifulSoup
import json
import time
from datetime import datetime
import sys
from tqdm import tqdm
import random

# è®¾ç½®æ§åˆ¶å°è¾“å‡ºç¼–ç ä¸ºUTF-8
sys.stdout.reconfigure(encoding='utf-8')

class TwitterCrawler:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        # nitterå®ä¾‹åˆ—è¡¨ - æŒ‰å¯é æ€§æ’åº
        self.instances = [
            'https://nitter.bird.froth.zone',
            'https://nitter.privacydev.net',
            'https://nitter.woodland.cafe',
            'https://nitter.ktachibana.party',
            'https://nitter.freedit.eu',
            'https://nitter.poast.org',
            'https://nitter.mint.lgbt',
            'https://nitter.fdn.fr',
            'https://nitter.1d4.us',
            'https://nitter.kavin.rocks',
            'https://nitter.unixfox.eu',
            'https://nitter.moomoo.me',
            'https://nitter.net',
            'https://nitter.cz',
        ]

    async def get_user_tweet(self, session, username, max_retries=3):
        # éšæœºæ‰“ä¹±å®ä¾‹åˆ—è¡¨
        instances = self.instances.copy()
        random.shuffle(instances)
        
        # ä¸ºæ¯ä¸ªå®ä¾‹å°è¯•å¤šæ¬¡
        for instance in instances:
            for retry in range(max_retries):
                try:
                    url = f'{instance}/{username}'
                    async with session.get(url, headers=self.headers, ssl=False, timeout=5) as response:
                        if response.status != 200:
                            print(f"âœ— {username}: HTTP {response.status} from {instance} (retry {retry + 1}/{max_retries})")
                            await asyncio.sleep(1)  # ç­‰å¾…1ç§’åé‡è¯•
                            continue

                        html = await response.text()
                        soup = BeautifulSoup(html, 'html.parser')
                        
                        # è·å–ç¬¬ä¸€æ¡æ¨æ–‡
                        tweet_container = soup.find('div', class_='timeline-item')
                        if not tweet_container:
                            print(f"âœ— {username}: No tweets found on {instance}")
                            continue

                        # è·å–æ¨æ–‡å†…å®¹
                        tweet = tweet_container.find('div', class_='tweet-content')
                        if not tweet:
                            continue

                        # è·å–æ¨æ–‡é“¾æ¥
                        link_elem = tweet_container.find('a', class_='tweet-link')
                        tweet_id = link_elem['href'].split('/')[-1] if link_elem else None
                        
                        content = tweet.get_text(strip=True)
                        
                        tweet_data = {
                            'ä½œè€…': username,
                            'å†…å®¹': content,
                            'æŠ“å–æ—¶é—´': time.strftime('%Y-%m-%d %H:%M:%S'),
                            'æ¥æº': instance,
                            'é“¾æ¥': f'https://twitter.com/{username}/status/{tweet_id}' if tweet_id else None
                        }
                        
                        print(f"âœ“ {username}: {content[:50]}...")
                        return tweet_data
                    
                except asyncio.TimeoutError:
                    print(f"âœ— {username}: Timeout on {instance} (retry {retry + 1}/{max_retries})")
                    await asyncio.sleep(1)
                    continue
                except Exception as e:
                    print(f"âœ— {username}: Error on {instance} - {str(e)} (retry {retry + 1}/{max_retries})")
                    await asyncio.sleep(1)
                    continue
        
        print(f"âœ— {username}: Failed on all instances after {max_retries} retries")
        return None

    async def get_all_tweets(self, users):
        # è®¾ç½®æ›´å®½æ¾çš„è¿æ¥é™åˆ¶
        conn = aiohttp.TCPConnector(limit=5, force_close=True, enable_cleanup_closed=True)
        timeout = aiohttp.ClientTimeout(total=60, connect=5, sock_connect=5, sock_read=5)
        
        async with aiohttp.ClientSession(connector=conn, timeout=timeout) as session:
            tasks = []
            for username in users:
                task = asyncio.create_task(self.get_user_tweet(session, username))
                tasks.append(task)
                await asyncio.sleep(0.5)  # æ·»åŠ å»¶è¿Ÿï¼Œé¿å…åŒæ—¶å‘èµ·å¤ªå¤šè¯·æ±‚
            
            results = await asyncio.gather(*tasks)
            return [r for r in results if r is not None]

    def save_tweets_info(self, tweets_info):
        # ä¿å­˜JSONæ–‡ä»¶
        with open('twitter_posts.json', 'w', encoding='utf-8') as f:
            json.dump(tweets_info, f, ensure_ascii=False, indent=2)
        
        # ç”ŸæˆHTMLå†…å®¹
        html_content = self.generate_html(tweets_info)
        with open('twitter.html', 'w', encoding='utf-8') as f:
            f.write(html_content)

    def generate_html(self, tweets):
        html = '''
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Twitter Latest Posts</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            background: #f5f5f5;
        }
        .header {
            text-align: center;
            margin-bottom: 30px;
            color: #333;
        }
        .tweet-list {
            display: flex;
            flex-direction: column;
            gap: 15px;
        }
        .tweet-card {
            background: white;
            border-radius: 10px;
            padding: 15px;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }
        .tweet-content {
            font-size: 16px;
            margin: 0 0 10px 0;
            color: #333;
        }
        .tweet-meta {
            font-size: 14px;
            color: #666;
            display: flex;
            justify-content: space-between;
            flex-wrap: wrap;
            gap: 10px;
        }
        .tweet-link {
            color: #1da1f2;
            text-decoration: none;
        }
        .tweet-link:hover {
            text-decoration: underline;
        }
        .update-time {
            text-align: center;
            color: #666;
            margin-top: 20px;
            font-size: 14px;
        }
    </style>
</head>
<body>
    <div class="header">
        <h1>Twitter Latest Posts</h1>
    </div>
    <div class="tweet-list">
'''
        
        for tweet in tweets:
            html += f'''
        <div class="tweet-card">
            <div class="tweet-content">{tweet['å†…å®¹']}</div>
            <div class="tweet-meta">
                <span>@{tweet['ä½œè€…']}</span>
                <span>{tweet['æŠ“å–æ—¶é—´']}</span>
                {'<a href="' + tweet['é“¾æ¥'] + '" target="_blank" class="tweet-link">æŸ¥çœ‹åŸæ–‡</a>' if tweet['é“¾æ¥'] else ''}
            </div>
        </div>'''

        html += f'''
    </div>
    <div class="update-time">
        æœ€åæ›´æ–°æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}
    </div>
</body>
</html>
'''
        return html

async def main():
    # Twitterç”¨æˆ·åˆ—è¡¨
    users = [
        "levelsio",      # Pieter Levels
        "AxtonLiu",      # Axton Liu
        "dotey",         # Dotey
        "mckaywrigley",  # McKay Wrigley
        "aigclink",      # AIGC Link
        "op7418"         # OP7418
    ]
    
    start_time = time.time()
    crawler = TwitterCrawler()
    
    print("å¼€å§‹è·å–æ¨æ–‡ä¿¡æ¯...")
    
    # ç¦ç”¨SSLéªŒè¯è­¦å‘Š
    import urllib3
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
    
    # å¼‚æ­¥è·å–æ‰€æœ‰æ¨æ–‡
    all_tweets = await crawler.get_all_tweets(users)
    
    end_time = time.time()
    duration = end_time - start_time
    
    if all_tweets:
        crawler.save_tweets_info(all_tweets)
        print(f"\nâœ… æˆåŠŸè·å– {len(all_tweets)} æ¡æ¨æ–‡")
        print(f"â±ï¸ æ€»ç”¨æ—¶: {duration:.2f} ç§’")
        print("ğŸ“ ç»“æœå·²ä¿å­˜åˆ° twitter_posts.json å’Œ twitter.html")
    else:
        print("\nâŒ æ²¡æœ‰è·å–åˆ°ä»»ä½•æ¨æ–‡")

if __name__ == "__main__":
    # Windowséœ€è¦ç‰¹åˆ«è®¾ç½®äº‹ä»¶å¾ªç¯ç­–ç•¥
    if sys.platform == 'win32':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(main())
